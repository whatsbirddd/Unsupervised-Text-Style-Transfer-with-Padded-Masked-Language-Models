{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.10.1+cu102].\n",
      "transformers version:[4.8.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "\n",
    "#check torch version & device\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "print (\"transformers version:[%s].\"%(transformers.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'datadir' : '../data/Part1',\n",
    "    'savedir' : '../data-styleT',\n",
    "    'model'   : 'KRBERT/pytorch_model_char16424_ranked.bin',\n",
    "    'config'  : 'KRBERT/bert_config.json',\n",
    "    'MODEL'   : {'max_seq_length' : 512},\n",
    "    'DATASET' : {'num_train_nonbait' : 20000,},\n",
    "    'SEED':42    \n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "- row로 일단 계산하고\n",
    "- 두번째 시도 : matrix로 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1628: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at KRBERT/pytorch_model_char16424_ranked.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('KRBERT/vocab.txt', do_lower_case=False)\n",
    "config = BertConfig(cfg[\"config\"])\n",
    "model = BertForMaskedLM.from_pretrained(cfg['model'], config=cfg['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "nonbait_filelist = glob(os.path.join(cfg['datadir'], '[!sample]*/Clickbait_Auto/*/*'))\n",
    "train_size = cfg['DATASET']['num_train_nonbait']\n",
    "inference_size = len(nonbait_filelist) - train_size\n",
    "nonbait_train, nonbait_infer = random_split(dataset = nonbait_filelist, \n",
    "                                            lengths = [train_size, inference_size], \n",
    "                                            generator = torch.Generator().manual_seed(42)\n",
    "                                            )\n",
    "nonbait_train_list = [nonbait_filelist[i] for i in nonbait_train.indices]\n",
    "nonbait_infer_list = [nonbait_filelist[i] for i in nonbait_infer.indices]\n",
    "\n",
    "bait_filelist = glob(os.path.join(cfg['datadir'], '[!sample]*/Clickbait_Direct/*/*'))\n",
    "file_list = nonbait_train_list + bait_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70131"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 불러오기\n",
    "class PaddedDataset(Dataset):\n",
    "    def __init__(self, file_list, tokenizer, max_seq_length, PAD = False):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_list = file_list\n",
    "        self.PAD = PAD\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self._get_text(self.file_list[idx])\n",
    "        source = self.tokenizer(input, max_length = self.max_seq_length, \n",
    "                                padding = \"max_length\", truncation = True, \n",
    "                                )\n",
    "        source_ids, target_ids = self.mask_token(source['input_ids'])\n",
    "\n",
    "        if 'Clickbait_Direct' in self.file_list[idx]:\n",
    "            token_type_ids = torch.tensor([1] * self.max_seq_length, dtype = torch.long)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor([0] * self.max_seq_length, dtype = torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(source_ids, dtype=torch.long), \n",
    "            \"attention_mask\" : torch.tensor(source['attention_mask'], dtype=torch.long), \n",
    "            \"token_type_ids\" : token_type_ids,\n",
    "            \"labels\" : torch.tensor(target_ids, dtype = torch.long)\n",
    "            }\n",
    "\n",
    "    def _get_text(self, file_path):\n",
    "        source_file = json.load(open(file_path, \"r\"))\n",
    "        title = source_file['sourceDataInfo']['newsTitle']\n",
    "        content = source_file['sourceDataInfo']['newsContent']\n",
    "        input_text = title + '[SEP]' + content\n",
    "        return input_text\n",
    "    \n",
    "    def mask_token(self, input_ids : list, n = 4): \n",
    "        \"\"\"\n",
    "        BertTokenizer [SEP]를 사용한다고 가정\n",
    "        input : w1, ..., wi-1, [MASK][MASK][MASK][MASK], wi+3, \n",
    "        label : \n",
    "        \"\"\"\n",
    "        label = input_ids.copy()\n",
    "\n",
    "        # 1. title 부분에 [MASK]처리하기\n",
    "        input_ids = np.array(input_ids)\n",
    "        content_idx = np.where(input_ids == self.tokenizer.sep_token_id)[0]\n",
    "\n",
    "        if self.PAD == False:\n",
    "            rand_idx = random.randint(1,content_idx[0]-n) #[CLS] w1, w2, ..., wk, [SEP]에서 [SEP]이 겹치지 않게 mask하기\n",
    "\n",
    "            ## input [MASK]처리하기\n",
    "            input_ids[rand_idx : rand_idx+n] = self.tokenizer.mask_token_id\n",
    "            label = np.array(label)\n",
    "            label[:rand_idx] = -100  # We only compute loss on masked tokens\n",
    "            label[rand_idx+n:] = -100\n",
    "\n",
    "        elif self.PAD == True :\n",
    "            ## 실제 mask 토큰의 개수(k) 구하기(1~4)\n",
    "            n_masked = random.randint(1, n)\n",
    "            rand_idx = random.randint(1,content_idx[0]-n_masked) #[CLS] w1, w2, ..., wk, [SEP]에서 [SEP]이 겹치지 않게 mask하기\n",
    "\n",
    "            ## input [MASK]처리하기\n",
    "            input_ids[rand_idx : rand_idx+n_masked] = self.tokenizer.mask_token_id\n",
    "\n",
    "            ## pad token추가 되는 부분까지 [MASK]추가하기\n",
    "            if n_masked != n :\n",
    "                input_ids = np.hstack((input_ids[:rand_idx+n_masked], np.full(n-n_masked, self.tokenizer.mask_token_id), \n",
    "                                    input_ids[rand_idx+n_masked:]))\n",
    "            ## label에 [PAD] 추가하기   \n",
    "            label = np.hstack((label[:rand_idx+n_masked], np.full(n-n_masked, self.tokenizer.pad_token_id),\n",
    "                            label[rand_idx+n_masked:],))\n",
    "\n",
    "            # 2. loss계산 안 할 부분 찾기 : special token(cls, sep) + content\n",
    "            label[:rand_idx] = -100  # We only compute loss on masked tokens\n",
    "            label[rand_idx+n:] = -100\n",
    "\n",
    "            ## maxlen 맞추기\n",
    "            input_ids = np.hstack((input_ids[:self.max_seq_length-1], [tokenizer.sep_token_id]))\n",
    "            label = label[:self.max_seq_length]\n",
    "\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(file_list))\n",
    "test_size = len(file_list) - train_size\n",
    "\n",
    "train_idx, test_idx = random_split(file_list, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_list=[file_list[i] for i in train_idx.indices]\n",
    "test_list=[file_list[i] for i in test_idx.indices]\n",
    "\n",
    "trainset = PaddedDataset(train_list, tokenizer, max_seq_length=512)\n",
    "testset = PaddedDataset(test_list, tokenizer, max_seq_length=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padded MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 80209\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100270\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     3/100270 00:00 < 13:27:10, 2.07 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m./model_output\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model_output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Token in source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112519"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(cfg['savedir'], 'infer.txt')\n",
    "file_list_infer = open(path, \"r\").read().split(\"\\n\")\n",
    "len(file_list_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"model_output/checkpoint-66000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file_path):\n",
    "    source_file = json.load(open(file_path, \"r\"))\n",
    "    title = source_file['sourceDataInfo']['newsTitle']\n",
    "    content = source_file['sourceDataInfo']['newsContent']\n",
    "    text = title + '[SEP]' + content\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit(file_path, cfg, model, span_size, SOURCE = None):\n",
    "    text = get_text(file_path)\n",
    "    input = tokenizer(text, \n",
    "                      max_length = cfg[\"MODEL\"][\"max_seq_length\"], \n",
    "                      padding = \"max_length\", \n",
    "                      truncation = True, \n",
    "                      return_tensors=\"pt\"\n",
    "                      )\n",
    "    if SOURCE :\n",
    "        token_type_ids = torch.tensor([0] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #source\n",
    "    else:\n",
    "        token_type_ids = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "    input[\"token_type_ids\"] = token_type_ids\n",
    "    with torch.no_grad():\n",
    "        output = model(**input).logits\n",
    "    indices = input.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "    logit_of_input_ids = torch.gather(output, 2, indices).squeeze() #(1, 512, 1) : torch.gather 좋네\n",
    "\n",
    "    ## input에서 [sep]의 index찾기\n",
    "    source_sep_id = (input.input_ids[0] == tokenizer.sep_token_id).nonzero().squeeze()[0] \n",
    "    ## [sep]나오기 전까지 span길이만큼의 logit 합으로 구성된 matrix구하기\n",
    "    n_gram_logits = torch.tensor([sum(logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])\n",
    "    return n_gram_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run(file_path, cfg, model, span_size):\n",
    "    s_n_gram_logits = get_logit(file_path, cfg, model, span_size=4, SOURCE = True)\n",
    "    t_n_gram_logits = get_logit(file_path, cfg, model, span_size=4, SOURCE = False)\n",
    "    \n",
    "    # span의 logit 차이가 큰 index부분 찾기 -> MASK할 부분\n",
    "    diff = s_n_gram_logits-t_n_gram_logits\n",
    "    mask_idx = diff.argmax() #source index로 사용하면 됨.\n",
    "\n",
    "    text = get_text(file_path) \n",
    "    label = tokenizer(text, \n",
    "                      max_length = cfg[\"MODEL\"][\"max_seq_length\"], \n",
    "                      padding = \"max_length\", \n",
    "                      truncation = True, \n",
    "                      return_tensors = \"pt\"\n",
    "                      )\n",
    "    masked_input = copy.deepcopy(label)\n",
    "    masked_input['input_ids'][0, mask_idx : mask_idx+span_size] = tokenizer.mask_token_id\n",
    "    masked_input['token_type_ids'] = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "    \n",
    "    return masked_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input, label = run(file_list_infer[0], cfg, model, span_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(input_list, label_list, savedir): \n",
    "    input_dict = {}\n",
    "    for i, input in enumerate(tqdm(input_list)):\n",
    "        if len(input_dict) == 0:\n",
    "            for k in input.keys():\n",
    "                input_dict[k] = []\n",
    "        \n",
    "        for k in input.keys():\n",
    "            input_dict[k].append(input[k])\n",
    "\n",
    "    for k in input_dict.keys():\n",
    "        input_dict[k] = torch.cat(input_dict[k])\n",
    "    label_list = torch.cat(label_list)\n",
    "\n",
    "    torch.save({'input':input_dict, 'label':label_list}, os.path.join(savedir,'infer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 9177.91it/s]\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "label_list = []\n",
    "for i, file_path in enumerate(file_list_infer[:2]):\n",
    "    masked_input, label = run(file_path, cfg, model, span_size=4)\n",
    "    input_list.append(masked_input)\n",
    "    label_list.append(label.input_ids)\n",
    "\n",
    "save(input_list, label_list, cfg[\"savedir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "data = torch.load(os.path.join(cfg[\"savedir\"], 'infer.pt'))\n",
    "input = {}\n",
    "for k in data['input'].keys():\n",
    "    input[k] = data['input'][k][i]\n",
    "label = data['label'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 대전 갈마동에 갤 [MASK] [MASK] [MASK] [MASK]움 ‘ 분양 ’ [SEP] [UNK] 다우주택건설이 대전 갈마동에 공급하는 301세대 갤러리휴리움 조감도. 다우주택건설 ( 주 ), 7일 모델하우스 개장 … 301세대 분양 학군 · 지하철 역세권 혜택 … 10년만의 대규모 공급 대전 서구 갈마동에 27층 높이 301세대 규모의 신규 아파트단지가 들어선다. 더욱이 갈마동 지역에 10년 만에 나온 아파트 공급으로써 초ㆍ중ㆍ고 등 완성된 학군과 지하철역세권 혜택을 누릴 것으로 전망되면서 벌써부터 관심을 모으고 있다. 다우주택건설 ( 주 ) 이 오는 7일 갈마동 ‘ 갤러리휴리움 ’ 모델하우스를 개장하고 본격적인 분양에 돌입한다. 전용면적 51㎡ 126세대, 57㎡ 62세대, 59㎡ 55세대, 65㎡ 28세대로 구성된 중소형 평형의 갤러리휴리움은 최고 27층 높이다. 갈마1동주민센터와 옛 백년예식장 맞은 편에 위치한 서구 갈마동 315번지 일원에 건설되는 갤러리휴리움은 생활여건이 완성된 지역에서 10년 만에 이뤄지는 아파트 공급이라는 점에서 관심을 끈다. 인근 지역에서 최근 분양된 규모 있는 아파트가 없고, 10년 만에 선보이는 300세대급 공급인 셈이다. 봉산초ㆍ갈마중ㆍ한밭고 등 주변에 학군이 형성돼 있고 월평역과 갈마역까지 걸어서 접근할 수 있는 역세권에 해당한다. 또 한밭대로와 계룡로에 가까워 대부분 대전지역에서 쉽게 접근할 수 있는 교통망을 갖춘 상태다. 최고 27층 높이로 설계돼 갈마동 인근 건물 중 가장 높은 층을 확보하게 돼 눈에 쉽게 들어오는 랜드마크로 자리매김할 것으로 전망된다. 지역에 아파트를 공급하는 다우주택건설은 1999년 창립해 대전과 세종에서 갤러리빌이라는 브랜드의 주상복합 등을 13차례 성공 분양한 바 있다. 이번 갈마동 갤러리휴리움은 14차 공급으로써 편리한 시스템 구성으로 쾌적함과 안전함을 고루 갖춘 단지를 만들겠다는 포부다. 다우주택건설 전문수 회장은 [UNK] 삶의 가치를 담은 갤러리휴리움 아파트를 갈마동에 선보여 최고이자 모범단지로서 랜드마크가 되도록 노력하겠다 [UNK] 고 [SEP]'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(masked_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 대전 갈마동에 갤러리휴리움 ‘ 분양 ’ [SEP] [UNK] 다우주택건설이 대전 갈마동에 공급하는 301세대 갤러리휴리움 조감도. 다우주택건설 ( 주 ), 7일 모델하우스 개장 … 301세대 분양 학군 · 지하철 역세권 혜택 … 10년만의 대규모 공급 대전 서구 갈마동에 27층 높이 301세대 규모의 신규 아파트단지가 들어선다. 더욱이 갈마동 지역에 10년 만에 나온 아파트 공급으로써 초ㆍ중ㆍ고 등 완성된 학군과 지하철역세권 혜택을 누릴 것으로 전망되면서 벌써부터 관심을 모으고 있다. 다우주택건설 ( 주 ) 이 오는 7일 갈마동 ‘ 갤러리휴리움 ’ 모델하우스를 개장하고 본격적인 분양에 돌입한다. 전용면적 51㎡ 126세대, 57㎡ 62세대, 59㎡ 55세대, 65㎡ 28세대로 구성된 중소형 평형의 갤러리휴리움은 최고 27층 높이다. 갈마1동주민센터와 옛 백년예식장 맞은 편에 위치한 서구 갈마동 315번지 일원에 건설되는 갤러리휴리움은 생활여건이 완성된 지역에서 10년 만에 이뤄지는 아파트 공급이라는 점에서 관심을 끈다. 인근 지역에서 최근 분양된 규모 있는 아파트가 없고, 10년 만에 선보이는 300세대급 공급인 셈이다. 봉산초ㆍ갈마중ㆍ한밭고 등 주변에 학군이 형성돼 있고 월평역과 갈마역까지 걸어서 접근할 수 있는 역세권에 해당한다. 또 한밭대로와 계룡로에 가까워 대부분 대전지역에서 쉽게 접근할 수 있는 교통망을 갖춘 상태다. 최고 27층 높이로 설계돼 갈마동 인근 건물 중 가장 높은 층을 확보하게 돼 눈에 쉽게 들어오는 랜드마크로 자리매김할 것으로 전망된다. 지역에 아파트를 공급하는 다우주택건설은 1999년 창립해 대전과 세종에서 갤러리빌이라는 브랜드의 주상복합 등을 13차례 성공 분양한 바 있다. 이번 갈마동 갤러리휴리움은 14차 공급으로써 편리한 시스템 구성으로 쾌적함과 안전함을 고루 갖춘 단지를 만들겠다는 포부다. 다우주택건설 전문수 회장은 [UNK] 삶의 가치를 담은 갤러리휴리움 아파트를 갈마동에 선보여 최고이자 모범단지로서 랜드마크가 되도록 노력하겠다 [UNK] 고 [SEP]'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(label['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input['token_type_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAFT 🚷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = get_text(file_list_infer[0])\n",
    "source = tokenizer(input, max_length = cfg[\"MODEL\"][\"max_seq_length\"], padding = \"max_length\", truncation = True, return_tensors=\"pt\")\n",
    "target = tokenizer(input, max_length = cfg[\"MODEL\"][\"max_seq_length\"], padding = \"max_length\", truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "token_type_ids = torch.tensor([0] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #source\n",
    "source[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "token_type_ids = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "target[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    s_outputs = model(**source).logits\n",
    "    t_outputs = model(**target).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indices = source.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "s_logit_of_input_ids = torch.gather(s_outputs, 2, s_indices).squeeze() #(1, 512, 1) : torch.gather 좋네\n",
    "\n",
    "t_indices = target.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "t_logit_of_input_ids = torch.gather(t_outputs, 2, s_indices).squeeze() #(1, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span길이만큼의 logit 합으로 구성된 matrix구하기\n",
    "span_size = 4\n",
    "source_sep_id = (source.input_ids[0] == tokenizer.sep_token_id).nonzero().squeeze()[0]\n",
    "s_n_gram_logits = torch.tensor([sum(s_logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])\n",
    "t_n_gram_logits = torch.tensor([sum(t_logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13)\n"
     ]
    }
   ],
   "source": [
    "# logit 차이가 가장 큰 span index 찾기\n",
    "diff = s_n_gram_logits-t_n_gram_logits\n",
    "mask_idx = diff.argmax() #source index로 사용하면 됨.\n",
    "print(mask_idx)\n",
    "\n",
    "# logit이 가장 큰 index에 masking하기\n",
    "source.input_ids[0, mask_idx : mask_idx+span_size] = tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  5050,  2342,     8,   516,  1889,     9,  1540,    28,    81,\n",
       "           670, 10011,    11,     4,     4,     4,     4, 10011,    11,     3,\n",
       "           440,   320,   113,  1284,    81,   219,    10,   159,   157,   607,\n",
       "          5136, 12137,   107,   124,  5050,  5136,   516,  1889,     9,  1896,\n",
       "            26,    30,     5,   741,  1612,   565,   159,   157,   607,  4208,\n",
       "             9,  6076,  6426,   879,   833,    78,  2342,     7,   366,  1612,\n",
       "          1087,   159,   157,   607,  5136,   301,    10,  1941,    87,    68,\n",
       "           284,  1340,   159,    16,  6034,  1027,  3005,     5,   159,   157,\n",
       "           607,  1540,  6426,  3555,  2817,    72,   266,  5285,  1153,   277,\n",
       "          2597,  1505,  3773,   440,   320,   113,    10,   912,   359,    32,\n",
       "          2270,  1427,  1540,     6,  1844,    13,  1043,  4974,     9,   658,\n",
       "           464,   681,  1436,   369,   605,    16,  1219,  6068,    16,  6402,\n",
       "          2232,  1599,    21,  1574,     5,  1540,  3817,    21,   285,    13,\n",
       "           501,   372,  2916,    22,   833,    78,  6289,    10,   388,   785,\n",
       "           757,  1540,     6,    87,    68,   128,  9925,    27,  7809,   278,\n",
       "          2088,  2236,    15,  2334,    20,   109,  1679,     5,   131,  6002,\n",
       "            28,   229,  3062,    24, 13914,   215,  5136,  7424,   170,   741,\n",
       "          1612,    69,   159,   157,   607,  4208,     9,  6076,  6426,  6215,\n",
       "            45,  1748,   878,     5,  1286,   333,     7,   366,   301,   741,\n",
       "          1612,   565,   159,   157,   607,  4208,     9,  6076,  6426,  3285,\n",
       "            68,   440,   109,  1679,     5,   660,    25,  1781,    68,   211,\n",
       "          2993,   508,  1991,  1941,   102,    68,  3585,     5,   367,    13,\n",
       "           741,  1612,    69,   159,   157,   607,  4208,     9,  6076,  3991,\n",
       "            15,  2741,   727,    68,   300,  1469,   446,   109,  6761,     5,\n",
       "          1286,   333,    15,   301,   229,  3062,     6,  3365,   146,   208,\n",
       "           303,  2312,    10,  1732,  1651,    64,   367,   120,   426,  1627,\n",
       "          2312,    19,   643,    68,   190,  6944,  1540,    10,  4319,  1147,\n",
       "             5, 13914,   215,     7,   366,   301,  3365,   146,  4299,  1295,\n",
       "          2312,    23,   367,   187,  1276,  2312,    19,   614,    68,  4035,\n",
       "          1896,   124,   159,    16,  6034,  5815,   139,  3021,   593,     5,\n",
       "           147,  7600,    41,     8,   677,   307,  5285,    57,    21,  5849,\n",
       "            28,  4269,     5,   147,  7600,    41,     7,   366,   301,   741,\n",
       "          1612,   565,   159,   157,   607,  4208,     9,  6076,  3991,    15,\n",
       "          4956,    68, 13348,  1209,   367,    43,  1220,   106,   159,   157,\n",
       "           607,  4208,     9,  6076,  3991,    15,  6212,    68,   234,  3021,\n",
       "           593,     5,   677,   307,  5285,     7,   366,   159,   157,   607,\n",
       "          4208,  6426,   301,  3717,    68,   613,   367,  1322,    68,   284,\n",
       "             6,   390,    47,   109,   839,     5,   455,   677,   307,  5285,\n",
       "             7,   366,   741,  1612,   565,   833,    78,     9,  6076,  6426,\n",
       "          2377,    68,   234,   757,  1540,  1219,   369,  3394,    15,  1655,\n",
       "            47,   109,  6761,     5,  2040,    21,  5390, 10433,  1540,    28,\n",
       "            81, 10434,   159,   157,   607,  4208,  1505,    10,  1338,  4264,\n",
       "           898,  5050,  1153,   277,  7424,    20,   440,   320,  3139,   321,\n",
       "            31,     5,  3514,   118,    22,   118,   146,     7,   366,   741,\n",
       "          1612,    69,  7105,    16,  6076,  3991,    15,   660,    25,   102,\n",
       "            68,   126,    23,  1561,    25,   117,    68,  6263,  1340,   301,\n",
       "           120,    87,    68,   300,  1469,  9977,    31,     5,  1612,  1505,\n",
       "          1255,   230,   440,   320,   113,  6805,  1255,     9,   140,  2872,\n",
       "           878,     5,   455,   440,   320,   113,     7,   366,  1612,    24,\n",
       "          7283,     9,  2597,  1896,    31,  3118,  4402,  1529,   398,    42,\n",
       "          1649,    15,   140,  2555,     5,   674,  1783,    13,   555,   497,\n",
       "            34,     3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 식품업계, 줄줄이 가격인상 … \\\\ \" [MASK] [MASK] [MASK] [MASK] \\\\ \" [SEP] 인건비 급상승에 원재료 가격이 겹치며 식품 가격이 줄줄이 오르고 있다. 전체 매출 가운데 원재료 값이 차지하는 비중이 높은 라면업계의 경우 매출 대비 원재료 가격이 지난해에 비해 10 % 이상 올라 원가 부담이 크게 늘었다. 원재료 가격 비중이 다소 낮은 가공식품업체들은 매년 상승해온 인건비에 버티지 못하고 상품 가격을 올리는 악순환이 계속되고 있어 내년에도 물가 인상 기조가 이어질 것이라는 전망도 나온다. 가격 올려도 남는 게 없다24일 라면 업체들에 따르면 주요 제품 가격을 10 % 안팎으로 올렸지만 여전히 실적은 부진한 것으로 나타났다. 주 재료인 소맥과 팜유 가격이 급등하며 전체 매출 중 원재료 값이 차지하는 비중이 급증하고 있기 때문이다. 농심의 경우 지난해 전체 매출 가운데 원재료 값이 차지하는 비중이 43 % 인 것으로 나타났다. 2018년 35 % 에 불과했던 것에 비해 8 % 증가했다. 올해는 전체 매출 중 원재료 값이 차지하는 비중은 최소 50 % 를 넘어설 것으로 전망된다. 농심은 지난해 소맥을 t당 202달러에 수입했으나 올해에는 258달러로 27 % 비싼 가격에 들여왔다. 팜유의 경우 지난해 t당 627달러에서 올해 1110달러로 100 % 가까이 오르며 원가 부담이 급격히 높아졌다. 오뚜기, 삼양식품 등도 마찬가지인 상황이다. 오뚜기의 경우 지난해 전체 매출 가운데 원재료 값이 차지하는 비중은 69 % 였으나 올해 3분기까지 원재료 값이 차지하는 비중은 81 % 로 높아졌다. 삼양식품의 경우 원재료 값 비중이 지난해 56 % 에서 올해 60 % 이상을 기록할 것으로 보인다. 특히 삼양식품의 경우 전체 매출 가운데 라면이 차지하는 비중이 90 % 로 제품 가격 인상에도 영업이익은 하락할 것으로 전망된다. 너도 나도 ‘ 가격인상 ’ 원재료 값 상승에 영향을 덜 받은 식품업체들은 급등한 인건비가 문제다. CJ제일제당의 경우 전체 매출 중 급여가 차지하는 비중은 2018년 8 % 대에서 2019년 9 % 대로 올라 지난해에는 10 % 를 넘어섰다. 매출 상승폭보다 인건비 지출폭이 더 크기 때문이다. 특히 인건비의 경우 매출과 상관없이 매년 오르다 보니 기업이 체감하는 부담은 더 크다. 롯데리아는 다음 달 1 [SEP]'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(source.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_gen = source.copy()\n",
    "data_for_gen[\"token_type_ids\"] = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**data_for_gen).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'내년도 [PAD] [PAD]'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve index of [MASK]\n",
    "mask_token_index = (data_for_gen.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = outputs[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id) #학습이 제대로 안됨... 나 어떠카지..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 식품업계, 줄줄이 가격인상 … \\\\ \" 남는 게 없다 \\\\ \" [SEP] 인건비 급상승에 원재료 가격이 겹치며 식품 가격이 줄줄이 오르고 있다. 전체 매출 가운데 원재료 값이 차지하는 비중이 높은 라면업계의 경우 매출 대비 원재료 가격이 지난해에 비해 10 % 이상 올라 원가 부담이 크게 늘었다. 원재료 가격 비중이 다소 낮은 가공식품업체들은 매년 상승해온 인건비에 버티지 못하고 상품 가격을 올리는 악순환이 계속되고 있어 내년에도 물가 인상 기조가 이어질 것이라는 전망도 나온다. 가격 올려도 남는 게 없다24일 라면 업체들에 따르면 주요 제품 가격을 10 % 안팎으로 올렸지만 여전히 실적은 부진한 것으로 나타났다. 주 재료인 소맥과 팜유 가격이 급등하며 전체 매출 중 원재료 값이 차지하는 비중이 급증하고 있기 때문이다. 농심의 경우 지난해 전체 매출 가운데 원재료 값이 차지하는 비중이 43 % 인 것으로 나타났다. 2018년 35 % 에 불과했던 것에 비해 8 % 증가했다. 올해는 전체 매출 중 원재료 값이 차지하는 비중은 최소 50 % 를 넘어설 것으로 전망된다. 농심은 지난해 소맥을 t당 202달러에 수입했으나 올해에는 258달러로 27 % 비싼 가격에 들여왔다. 팜유의 경우 지난해 t당 627달러에서 올해 1110달러로 100 % 가까이 오르며 원가 부담이 급격히 높아졌다. 오뚜기, 삼양식품 등도 마찬가지인 상황이다. 오뚜기의 경우 지난해 전체 매출 가운데 원재료 값이 차지하는 비중은 69 % 였으나 올해 3분기까지 원재료 값이 차지하는 비중은 81 % 로 높아졌다. 삼양식품의 경우 원재료 값 비중이 지난해 56 % 에서 올해 60 % 이상을 기록할 것으로 보인다. 특히 삼양식품의 경우 전체 매출 가운데 라면이 차지하는 비중이 90 % 로 제품 가격 인상에도 영업이익은 하락할 것으로 전망된다. 너도 나도 ‘ 가격인상 ’ 원재료 값 상승에 영향을 덜 받은 식품업체들은 급등한 인건비가 문제다. CJ제일제당의 경우 전체 매출 중 급여가 차지하는 비중은 2018년 8 % 대에서 2019년 9 % 대로 올라 지난해에는 10 % 를 넘어섰다. 매출 상승폭보다 인건비 지출폭이 더 크기 때문이다. 특히 인건비의 경우 매출과 상관없이 매년 오르다 보니 기업이 체감하는 부담은 더 크다. 롯데리아는 다음 달 1 [SEP]'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (17) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m labels \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39mThe capital of France is Paris.\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# mask labels of non-[MASK] tokens\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(target\u001b[39m.\u001b[39;49minput_ids[\u001b[39m0\u001b[39;49m] \u001b[39m==\u001b[39;49m tokenizer\u001b[39m.\u001b[39;49mmask_token_id, labels, \u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtarget, labels\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mround\u001b[39m(outputs\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mitem(), \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (17) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(target.input_ids[0] == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**target, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
