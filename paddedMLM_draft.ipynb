{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.10.1+cu102].\n",
      "transformers version:[4.8.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "\n",
    "#check torch version & device\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "print (\"transformers version:[%s].\"%(transformers.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'datadir' : '../data/Part1',\n",
    "    'savedir' : '../data-styleT',\n",
    "    'model'   : 'KRBERT/pytorch_model_char16424_ranked.bin',\n",
    "    'config'  : 'KRBERT/bert_config.json',\n",
    "    'MODEL'   : {'max_seq_length' : 512},\n",
    "    'DATASET' : {'num_train_nonbait' : 20000,},\n",
    "    'SEED':42    \n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "- rowÎ°ú ÏùºÎã® Í≥ÑÏÇ∞ÌïòÍ≥†\n",
    "- ÎëêÎ≤àÏß∏ ÏãúÎèÑ : matrixÎ°ú Í≥ÑÏÇ∞ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1628: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at KRBERT/pytorch_model_char16424_ranked.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('KRBERT/vocab.txt', do_lower_case=False)\n",
    "config = BertConfig(cfg[\"config\"])\n",
    "model = BertForMaskedLM.from_pretrained(cfg['model'], config=cfg['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "nonbait_filelist = glob(os.path.join(cfg['datadir'], '[!sample]*/Clickbait_Auto/*/*'))\n",
    "train_size = cfg['DATASET']['num_train_nonbait']\n",
    "inference_size = len(nonbait_filelist) - train_size\n",
    "nonbait_train, nonbait_infer = random_split(dataset = nonbait_filelist, \n",
    "                                            lengths = [train_size, inference_size], \n",
    "                                            generator = torch.Generator().manual_seed(42)\n",
    "                                            )\n",
    "nonbait_train_list = [nonbait_filelist[i] for i in nonbait_train.indices]\n",
    "nonbait_infer_list = [nonbait_filelist[i] for i in nonbait_infer.indices]\n",
    "\n",
    "bait_filelist = glob(os.path.join(cfg['datadir'], '[!sample]*/Clickbait_Direct/*/*'))\n",
    "file_list = nonbait_train_list + bait_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70131"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "class PaddedDataset(Dataset):\n",
    "    def __init__(self, file_list, tokenizer, max_seq_length, PAD = False):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_list = file_list\n",
    "        self.PAD = PAD\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self._get_text(self.file_list[idx])\n",
    "        source = self.tokenizer(input, max_length = self.max_seq_length, \n",
    "                                padding = \"max_length\", truncation = True, \n",
    "                                )\n",
    "        source_ids, target_ids = self.mask_token(source['input_ids'])\n",
    "\n",
    "        if 'Clickbait_Direct' in self.file_list[idx]:\n",
    "            token_type_ids = torch.tensor([1] * self.max_seq_length, dtype = torch.long)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor([0] * self.max_seq_length, dtype = torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(source_ids, dtype=torch.long), \n",
    "            \"attention_mask\" : torch.tensor(source['attention_mask'], dtype=torch.long), \n",
    "            \"token_type_ids\" : token_type_ids,\n",
    "            \"labels\" : torch.tensor(target_ids, dtype = torch.long)\n",
    "            }\n",
    "\n",
    "    def _get_text(self, file_path):\n",
    "        source_file = json.load(open(file_path, \"r\"))\n",
    "        title = source_file['sourceDataInfo']['newsTitle']\n",
    "        content = source_file['sourceDataInfo']['newsContent']\n",
    "        input_text = title + '[SEP]' + content\n",
    "        return input_text\n",
    "    \n",
    "    def mask_token(self, input_ids : list, n = 4): \n",
    "        \"\"\"\n",
    "        BertTokenizer [SEP]Î•º ÏÇ¨Ïö©ÌïúÎã§Í≥† Í∞ÄÏ†ï\n",
    "        input : w1, ..., wi-1, [MASK][MASK][MASK][MASK], wi+3, \n",
    "        label : \n",
    "        \"\"\"\n",
    "        label = input_ids.copy()\n",
    "\n",
    "        # 1. title Î∂ÄÎ∂ÑÏóê [MASK]Ï≤òÎ¶¨ÌïòÍ∏∞\n",
    "        input_ids = np.array(input_ids)\n",
    "        content_idx = np.where(input_ids == self.tokenizer.sep_token_id)[0]\n",
    "\n",
    "        if self.PAD == False:\n",
    "            rand_idx = random.randint(1,content_idx[0]-n) #[CLS] w1, w2, ..., wk, [SEP]ÏóêÏÑú [SEP]Ïù¥ Í≤πÏπòÏßÄ ÏïäÍ≤å maskÌïòÍ∏∞\n",
    "\n",
    "            ## input [MASK]Ï≤òÎ¶¨ÌïòÍ∏∞\n",
    "            input_ids[rand_idx : rand_idx+n] = self.tokenizer.mask_token_id\n",
    "            label = np.array(label)\n",
    "            label[:rand_idx] = -100  # We only compute loss on masked tokens\n",
    "            label[rand_idx+n:] = -100\n",
    "\n",
    "        elif self.PAD == True :\n",
    "            ## Ïã§Ï†ú mask ÌÜ†ÌÅ∞Ïùò Í∞úÏàò(k) Íµ¨ÌïòÍ∏∞(1~4)\n",
    "            n_masked = random.randint(1, n)\n",
    "            rand_idx = random.randint(1,content_idx[0]-n_masked) #[CLS] w1, w2, ..., wk, [SEP]ÏóêÏÑú [SEP]Ïù¥ Í≤πÏπòÏßÄ ÏïäÍ≤å maskÌïòÍ∏∞\n",
    "\n",
    "            ## input [MASK]Ï≤òÎ¶¨ÌïòÍ∏∞\n",
    "            input_ids[rand_idx : rand_idx+n_masked] = self.tokenizer.mask_token_id\n",
    "\n",
    "            ## pad tokenÏ∂îÍ∞Ä ÎêòÎäî Î∂ÄÎ∂ÑÍπåÏßÄ [MASK]Ï∂îÍ∞ÄÌïòÍ∏∞\n",
    "            if n_masked != n :\n",
    "                input_ids = np.hstack((input_ids[:rand_idx+n_masked], np.full(n-n_masked, self.tokenizer.mask_token_id), \n",
    "                                    input_ids[rand_idx+n_masked:]))\n",
    "            ## labelÏóê [PAD] Ï∂îÍ∞ÄÌïòÍ∏∞   \n",
    "            label = np.hstack((label[:rand_idx+n_masked], np.full(n-n_masked, self.tokenizer.pad_token_id),\n",
    "                            label[rand_idx+n_masked:],))\n",
    "\n",
    "            # 2. lossÍ≥ÑÏÇ∞ Ïïà Ìï† Î∂ÄÎ∂Ñ Ï∞æÍ∏∞ : special token(cls, sep) + content\n",
    "            label[:rand_idx] = -100  # We only compute loss on masked tokens\n",
    "            label[rand_idx+n:] = -100\n",
    "\n",
    "            ## maxlen ÎßûÏ∂îÍ∏∞\n",
    "            input_ids = np.hstack((input_ids[:self.max_seq_length-1], [tokenizer.sep_token_id]))\n",
    "            label = label[:self.max_seq_length]\n",
    "\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(file_list))\n",
    "test_size = len(file_list) - train_size\n",
    "\n",
    "train_idx, test_idx = random_split(file_list, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_list=[file_list[i] for i in train_idx.indices]\n",
    "test_list=[file_list[i] for i in test_idx.indices]\n",
    "\n",
    "trainset = PaddedDataset(train_list, tokenizer, max_seq_length=512)\n",
    "testset = PaddedDataset(test_list, tokenizer, max_seq_length=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padded MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 80209\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100270\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     3/100270 00:00 < 13:27:10, 2.07 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m./model_output\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model_output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Token in source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112519"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(cfg['savedir'], 'infer.txt')\n",
    "file_list_infer = open(path, \"r\").read().split(\"\\n\")\n",
    "len(file_list_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"model_output/checkpoint-66000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file_path):\n",
    "    source_file = json.load(open(file_path, \"r\"))\n",
    "    title = source_file['sourceDataInfo']['newsTitle']\n",
    "    content = source_file['sourceDataInfo']['newsContent']\n",
    "    text = title + '[SEP]' + content\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit(file_path, cfg, model, span_size, SOURCE = None):\n",
    "    text = get_text(file_path)\n",
    "    input = tokenizer(text, \n",
    "                      max_length = cfg[\"MODEL\"][\"max_seq_length\"], \n",
    "                      padding = \"max_length\", \n",
    "                      truncation = True, \n",
    "                      return_tensors=\"pt\"\n",
    "                      )\n",
    "    if SOURCE :\n",
    "        token_type_ids = torch.tensor([0] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #source\n",
    "    else:\n",
    "        token_type_ids = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "    input[\"token_type_ids\"] = token_type_ids\n",
    "    with torch.no_grad():\n",
    "        output = model(**input).logits\n",
    "    indices = input.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "    logit_of_input_ids = torch.gather(output, 2, indices).squeeze() #(1, 512, 1) : torch.gather Ï¢ãÎÑ§\n",
    "\n",
    "    ## inputÏóêÏÑú [sep]Ïùò indexÏ∞æÍ∏∞\n",
    "    source_sep_id = (input.input_ids[0] == tokenizer.sep_token_id).nonzero().squeeze()[0] \n",
    "    ## [sep]ÎÇòÏò§Í∏∞ Ï†ÑÍπåÏßÄ spanÍ∏∏Ïù¥ÎßåÌÅºÏùò logit Ìï©ÏúºÎ°ú Íµ¨ÏÑ±Îêú matrixÍµ¨ÌïòÍ∏∞\n",
    "    n_gram_logits = torch.tensor([sum(logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])\n",
    "    return n_gram_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run(file_path, cfg, model, span_size):\n",
    "    s_n_gram_logits = get_logit(file_path, cfg, model, span_size=4, SOURCE = True)\n",
    "    t_n_gram_logits = get_logit(file_path, cfg, model, span_size=4, SOURCE = False)\n",
    "    \n",
    "    # spanÏùò logit Ï∞®Ïù¥Í∞Ä ÌÅ∞ indexÎ∂ÄÎ∂Ñ Ï∞æÍ∏∞ -> MASKÌï† Î∂ÄÎ∂Ñ\n",
    "    diff = s_n_gram_logits-t_n_gram_logits\n",
    "    mask_idx = diff.argmax() #source indexÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ Îê®.\n",
    "\n",
    "    text = get_text(file_path) \n",
    "    label = tokenizer(text, \n",
    "                      max_length = cfg[\"MODEL\"][\"max_seq_length\"], \n",
    "                      padding = \"max_length\", \n",
    "                      truncation = True, \n",
    "                      return_tensors = \"pt\"\n",
    "                      )\n",
    "    masked_input = copy.deepcopy(label)\n",
    "    masked_input['input_ids'][0, mask_idx : mask_idx+span_size] = tokenizer.mask_token_id\n",
    "    masked_input['token_type_ids'] = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "    \n",
    "    return masked_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_input, label = run(file_list_infer[0], cfg, model, span_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(input_list, label_list, savedir): \n",
    "    input_dict = {}\n",
    "    for i, input in enumerate(tqdm(input_list)):\n",
    "        if len(input_dict) == 0:\n",
    "            for k in input.keys():\n",
    "                input_dict[k] = []\n",
    "        \n",
    "        for k in input.keys():\n",
    "            input_dict[k].append(input[k])\n",
    "\n",
    "    for k in input_dict.keys():\n",
    "        input_dict[k] = torch.cat(input_dict[k])\n",
    "    label_list = torch.cat(label_list)\n",
    "\n",
    "    torch.save({'input':input_dict, 'label':label_list}, os.path.join(savedir,'infer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 9177.91it/s]\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "label_list = []\n",
    "for i, file_path in enumerate(file_list_infer[:2]):\n",
    "    masked_input, label = run(file_path, cfg, model, span_size=4)\n",
    "    input_list.append(masked_input)\n",
    "    label_list.append(label.input_ids)\n",
    "\n",
    "save(input_list, label_list, cfg[\"savedir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "data = torch.load(os.path.join(cfg[\"savedir\"], 'infer.pt'))\n",
    "input = {}\n",
    "for k in data['input'].keys():\n",
    "    input[k] = data['input'][k][i]\n",
    "label = data['label'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ÎåÄÏ†Ñ Í∞àÎßàÎèôÏóê Í∞§ [MASK] [MASK] [MASK] [MASK]ÏõÄ ‚Äò Î∂ÑÏñë ‚Äô [SEP] [UNK] Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§Ïù¥ ÎåÄÏ†Ñ Í∞àÎßàÎèôÏóê Í≥µÍ∏âÌïòÎäî 301ÏÑ∏ÎåÄ Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ Ï°∞Í∞êÎèÑ. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ ( Ï£º ), 7Ïùº Î™®Îç∏ÌïòÏö∞Ïä§ Í∞úÏû• ‚Ä¶ 301ÏÑ∏ÎåÄ Î∂ÑÏñë ÌïôÍµ∞ ¬∑ ÏßÄÌïòÏ≤† Ïó≠ÏÑ∏Í∂å ÌòúÌÉù ‚Ä¶ 10ÎÖÑÎßåÏùò ÎåÄÍ∑úÎ™® Í≥µÍ∏â ÎåÄÏ†Ñ ÏÑúÍµ¨ Í∞àÎßàÎèôÏóê 27Ï∏µ ÎÜíÏù¥ 301ÏÑ∏ÎåÄ Í∑úÎ™®Ïùò Ïã†Í∑ú ÏïÑÌååÌä∏Îã®ÏßÄÍ∞Ä Îì§Ïñ¥ÏÑ†Îã§. ÎçîÏö±Ïù¥ Í∞àÎßàÎèô ÏßÄÏó≠Ïóê 10ÎÖÑ ÎßåÏóê ÎÇòÏò® ÏïÑÌååÌä∏ Í≥µÍ∏âÏúºÎ°úÏç® Ï¥à„ÜçÏ§ë„ÜçÍ≥† Îì± ÏôÑÏÑ±Îêú ÌïôÍµ∞Í≥º ÏßÄÌïòÏ≤†Ïó≠ÏÑ∏Í∂å ÌòúÌÉùÏùÑ ÎàÑÎ¶¥ Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêòÎ©¥ÏÑú Î≤åÏç®Î∂ÄÌÑ∞ Í¥ÄÏã¨ÏùÑ Î™®ÏúºÍ≥† ÏûàÎã§. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ ( Ï£º ) Ïù¥ Ïò§Îäî 7Ïùº Í∞àÎßàÎèô ‚Äò Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ ‚Äô Î™®Îç∏ÌïòÏö∞Ïä§Î•º Í∞úÏû•ÌïòÍ≥† Î≥∏Í≤©Ï†ÅÏù∏ Î∂ÑÏñëÏóê ÎèåÏûÖÌïúÎã§. Ï†ÑÏö©Î©¥Ï†Å 51„é° 126ÏÑ∏ÎåÄ, 57„é° 62ÏÑ∏ÎåÄ, 59„é° 55ÏÑ∏ÎåÄ, 65„é° 28ÏÑ∏ÎåÄÎ°ú Íµ¨ÏÑ±Îêú Ï§ëÏÜåÌòï ÌèâÌòïÏùò Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ ÏµúÍ≥† 27Ï∏µ ÎÜíÏù¥Îã§. Í∞àÎßà1ÎèôÏ£ºÎØºÏÑºÌÑ∞ÏôÄ Ïòõ Î∞±ÎÖÑÏòàÏãùÏû• ÎßûÏùÄ Ìé∏Ïóê ÏúÑÏπòÌïú ÏÑúÍµ¨ Í∞àÎßàÎèô 315Î≤àÏßÄ ÏùºÏõêÏóê Í±¥ÏÑ§ÎêòÎäî Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ ÏÉùÌôúÏó¨Í±¥Ïù¥ ÏôÑÏÑ±Îêú ÏßÄÏó≠ÏóêÏÑú 10ÎÖÑ ÎßåÏóê Ïù¥Î§ÑÏßÄÎäî ÏïÑÌååÌä∏ Í≥µÍ∏âÏù¥ÎùºÎäî Ï†êÏóêÏÑú Í¥ÄÏã¨ÏùÑ ÎÅàÎã§. Ïù∏Í∑º ÏßÄÏó≠ÏóêÏÑú ÏµúÍ∑º Î∂ÑÏñëÎêú Í∑úÎ™® ÏûàÎäî ÏïÑÌååÌä∏Í∞Ä ÏóÜÍ≥†, 10ÎÖÑ ÎßåÏóê ÏÑ†Î≥¥Ïù¥Îäî 300ÏÑ∏ÎåÄÍ∏â Í≥µÍ∏âÏù∏ ÏÖàÏù¥Îã§. Î¥âÏÇ∞Ï¥à„ÜçÍ∞àÎßàÏ§ë„ÜçÌïúÎ∞≠Í≥† Îì± Ï£ºÎ≥ÄÏóê ÌïôÍµ∞Ïù¥ ÌòïÏÑ±Îèº ÏûàÍ≥† ÏõîÌèâÏó≠Í≥º Í∞àÎßàÏó≠ÍπåÏßÄ Í±∏Ïñ¥ÏÑú Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî Ïó≠ÏÑ∏Í∂åÏóê Ìï¥ÎãπÌïúÎã§. Îòê ÌïúÎ∞≠ÎåÄÎ°úÏôÄ Í≥ÑÎ£°Î°úÏóê Í∞ÄÍπåÏõå ÎåÄÎ∂ÄÎ∂Ñ ÎåÄÏ†ÑÏßÄÏó≠ÏóêÏÑú ÏâΩÍ≤å Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî ÍµêÌÜµÎßùÏùÑ Í∞ñÏ∂ò ÏÉÅÌÉúÎã§. ÏµúÍ≥† 27Ï∏µ ÎÜíÏù¥Î°ú ÏÑ§Í≥ÑÎèº Í∞àÎßàÎèô Ïù∏Í∑º Í±¥Î¨º Ï§ë Í∞ÄÏû• ÎÜíÏùÄ Ï∏µÏùÑ ÌôïÎ≥¥ÌïòÍ≤å Îèº ÎààÏóê ÏâΩÍ≤å Îì§Ïñ¥Ïò§Îäî ÎûúÎìúÎßàÌÅ¨Î°ú ÏûêÎ¶¨Îß§ÍπÄÌï† Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÏßÄÏó≠Ïóê ÏïÑÌååÌä∏Î•º Í≥µÍ∏âÌïòÎäî Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ÏùÄ 1999ÎÖÑ Ï∞ΩÎ¶ΩÌï¥ ÎåÄÏ†ÑÍ≥º ÏÑ∏Ï¢ÖÏóêÏÑú Í∞§Îü¨Î¶¨ÎπåÏù¥ÎùºÎäî Î∏åÎûúÎìúÏùò Ï£ºÏÉÅÎ≥µÌï© Îì±ÏùÑ 13Ï∞®Î°Ä ÏÑ±Í≥µ Î∂ÑÏñëÌïú Î∞î ÏûàÎã§. Ïù¥Î≤à Í∞àÎßàÎèô Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ 14Ï∞® Í≥µÍ∏âÏúºÎ°úÏç® Ìé∏Î¶¨Ìïú ÏãúÏä§ÌÖú Íµ¨ÏÑ±ÏúºÎ°ú ÏæåÏ†ÅÌï®Í≥º ÏïàÏ†ÑÌï®ÏùÑ Í≥†Î£® Í∞ñÏ∂ò Îã®ÏßÄÎ•º ÎßåÎì§Í≤†Îã§Îäî Ìè¨Î∂ÄÎã§. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ Ï†ÑÎ¨∏Ïàò ÌöåÏû•ÏùÄ [UNK] ÏÇ∂Ïùò Í∞ÄÏπòÎ•º Îã¥ÏùÄ Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ ÏïÑÌååÌä∏Î•º Í∞àÎßàÎèôÏóê ÏÑ†Î≥¥Ïó¨ ÏµúÍ≥†Ïù¥Ïûê Î™®Î≤îÎã®ÏßÄÎ°úÏÑú ÎûúÎìúÎßàÌÅ¨Í∞Ä ÎêòÎèÑÎ°ù ÎÖ∏Î†•ÌïòÍ≤†Îã§ [UNK] Í≥† [SEP]'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(masked_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ÎåÄÏ†Ñ Í∞àÎßàÎèôÏóê Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ ‚Äò Î∂ÑÏñë ‚Äô [SEP] [UNK] Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§Ïù¥ ÎåÄÏ†Ñ Í∞àÎßàÎèôÏóê Í≥µÍ∏âÌïòÎäî 301ÏÑ∏ÎåÄ Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ Ï°∞Í∞êÎèÑ. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ ( Ï£º ), 7Ïùº Î™®Îç∏ÌïòÏö∞Ïä§ Í∞úÏû• ‚Ä¶ 301ÏÑ∏ÎåÄ Î∂ÑÏñë ÌïôÍµ∞ ¬∑ ÏßÄÌïòÏ≤† Ïó≠ÏÑ∏Í∂å ÌòúÌÉù ‚Ä¶ 10ÎÖÑÎßåÏùò ÎåÄÍ∑úÎ™® Í≥µÍ∏â ÎåÄÏ†Ñ ÏÑúÍµ¨ Í∞àÎßàÎèôÏóê 27Ï∏µ ÎÜíÏù¥ 301ÏÑ∏ÎåÄ Í∑úÎ™®Ïùò Ïã†Í∑ú ÏïÑÌååÌä∏Îã®ÏßÄÍ∞Ä Îì§Ïñ¥ÏÑ†Îã§. ÎçîÏö±Ïù¥ Í∞àÎßàÎèô ÏßÄÏó≠Ïóê 10ÎÖÑ ÎßåÏóê ÎÇòÏò® ÏïÑÌååÌä∏ Í≥µÍ∏âÏúºÎ°úÏç® Ï¥à„ÜçÏ§ë„ÜçÍ≥† Îì± ÏôÑÏÑ±Îêú ÌïôÍµ∞Í≥º ÏßÄÌïòÏ≤†Ïó≠ÏÑ∏Í∂å ÌòúÌÉùÏùÑ ÎàÑÎ¶¥ Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêòÎ©¥ÏÑú Î≤åÏç®Î∂ÄÌÑ∞ Í¥ÄÏã¨ÏùÑ Î™®ÏúºÍ≥† ÏûàÎã§. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ ( Ï£º ) Ïù¥ Ïò§Îäî 7Ïùº Í∞àÎßàÎèô ‚Äò Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ ‚Äô Î™®Îç∏ÌïòÏö∞Ïä§Î•º Í∞úÏû•ÌïòÍ≥† Î≥∏Í≤©Ï†ÅÏù∏ Î∂ÑÏñëÏóê ÎèåÏûÖÌïúÎã§. Ï†ÑÏö©Î©¥Ï†Å 51„é° 126ÏÑ∏ÎåÄ, 57„é° 62ÏÑ∏ÎåÄ, 59„é° 55ÏÑ∏ÎåÄ, 65„é° 28ÏÑ∏ÎåÄÎ°ú Íµ¨ÏÑ±Îêú Ï§ëÏÜåÌòï ÌèâÌòïÏùò Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ ÏµúÍ≥† 27Ï∏µ ÎÜíÏù¥Îã§. Í∞àÎßà1ÎèôÏ£ºÎØºÏÑºÌÑ∞ÏôÄ Ïòõ Î∞±ÎÖÑÏòàÏãùÏû• ÎßûÏùÄ Ìé∏Ïóê ÏúÑÏπòÌïú ÏÑúÍµ¨ Í∞àÎßàÎèô 315Î≤àÏßÄ ÏùºÏõêÏóê Í±¥ÏÑ§ÎêòÎäî Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ ÏÉùÌôúÏó¨Í±¥Ïù¥ ÏôÑÏÑ±Îêú ÏßÄÏó≠ÏóêÏÑú 10ÎÖÑ ÎßåÏóê Ïù¥Î§ÑÏßÄÎäî ÏïÑÌååÌä∏ Í≥µÍ∏âÏù¥ÎùºÎäî Ï†êÏóêÏÑú Í¥ÄÏã¨ÏùÑ ÎÅàÎã§. Ïù∏Í∑º ÏßÄÏó≠ÏóêÏÑú ÏµúÍ∑º Î∂ÑÏñëÎêú Í∑úÎ™® ÏûàÎäî ÏïÑÌååÌä∏Í∞Ä ÏóÜÍ≥†, 10ÎÖÑ ÎßåÏóê ÏÑ†Î≥¥Ïù¥Îäî 300ÏÑ∏ÎåÄÍ∏â Í≥µÍ∏âÏù∏ ÏÖàÏù¥Îã§. Î¥âÏÇ∞Ï¥à„ÜçÍ∞àÎßàÏ§ë„ÜçÌïúÎ∞≠Í≥† Îì± Ï£ºÎ≥ÄÏóê ÌïôÍµ∞Ïù¥ ÌòïÏÑ±Îèº ÏûàÍ≥† ÏõîÌèâÏó≠Í≥º Í∞àÎßàÏó≠ÍπåÏßÄ Í±∏Ïñ¥ÏÑú Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî Ïó≠ÏÑ∏Í∂åÏóê Ìï¥ÎãπÌïúÎã§. Îòê ÌïúÎ∞≠ÎåÄÎ°úÏôÄ Í≥ÑÎ£°Î°úÏóê Í∞ÄÍπåÏõå ÎåÄÎ∂ÄÎ∂Ñ ÎåÄÏ†ÑÏßÄÏó≠ÏóêÏÑú ÏâΩÍ≤å Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî ÍµêÌÜµÎßùÏùÑ Í∞ñÏ∂ò ÏÉÅÌÉúÎã§. ÏµúÍ≥† 27Ï∏µ ÎÜíÏù¥Î°ú ÏÑ§Í≥ÑÎèº Í∞àÎßàÎèô Ïù∏Í∑º Í±¥Î¨º Ï§ë Í∞ÄÏû• ÎÜíÏùÄ Ï∏µÏùÑ ÌôïÎ≥¥ÌïòÍ≤å Îèº ÎààÏóê ÏâΩÍ≤å Îì§Ïñ¥Ïò§Îäî ÎûúÎìúÎßàÌÅ¨Î°ú ÏûêÎ¶¨Îß§ÍπÄÌï† Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÏßÄÏó≠Ïóê ÏïÑÌååÌä∏Î•º Í≥µÍ∏âÌïòÎäî Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ÏùÄ 1999ÎÖÑ Ï∞ΩÎ¶ΩÌï¥ ÎåÄÏ†ÑÍ≥º ÏÑ∏Ï¢ÖÏóêÏÑú Í∞§Îü¨Î¶¨ÎπåÏù¥ÎùºÎäî Î∏åÎûúÎìúÏùò Ï£ºÏÉÅÎ≥µÌï© Îì±ÏùÑ 13Ï∞®Î°Ä ÏÑ±Í≥µ Î∂ÑÏñëÌïú Î∞î ÏûàÎã§. Ïù¥Î≤à Í∞àÎßàÎèô Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄÏùÄ 14Ï∞® Í≥µÍ∏âÏúºÎ°úÏç® Ìé∏Î¶¨Ìïú ÏãúÏä§ÌÖú Íµ¨ÏÑ±ÏúºÎ°ú ÏæåÏ†ÅÌï®Í≥º ÏïàÏ†ÑÌï®ÏùÑ Í≥†Î£® Í∞ñÏ∂ò Îã®ÏßÄÎ•º ÎßåÎì§Í≤†Îã§Îäî Ìè¨Î∂ÄÎã§. Îã§Ïö∞Ï£ºÌÉùÍ±¥ÏÑ§ Ï†ÑÎ¨∏Ïàò ÌöåÏû•ÏùÄ [UNK] ÏÇ∂Ïùò Í∞ÄÏπòÎ•º Îã¥ÏùÄ Í∞§Îü¨Î¶¨Ìú¥Î¶¨ÏõÄ ÏïÑÌååÌä∏Î•º Í∞àÎßàÎèôÏóê ÏÑ†Î≥¥Ïó¨ ÏµúÍ≥†Ïù¥Ïûê Î™®Î≤îÎã®ÏßÄÎ°úÏÑú ÎûúÎìúÎßàÌÅ¨Í∞Ä ÎêòÎèÑÎ°ù ÎÖ∏Î†•ÌïòÍ≤†Îã§ [UNK] Í≥† [SEP]'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(label['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input['token_type_ids']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAFT üö∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = get_text(file_list_infer[0])\n",
    "source = tokenizer(input, max_length = cfg[\"MODEL\"][\"max_seq_length\"], padding = \"max_length\", truncation = True, return_tensors=\"pt\")\n",
    "target = tokenizer(input, max_length = cfg[\"MODEL\"][\"max_seq_length\"], padding = \"max_length\", truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "token_type_ids = torch.tensor([0] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #source\n",
    "source[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "token_type_ids = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target\n",
    "target[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    s_outputs = model(**source).logits\n",
    "    t_outputs = model(**target).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indices = source.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "s_logit_of_input_ids = torch.gather(s_outputs, 2, s_indices).squeeze() #(1, 512, 1) : torch.gather Ï¢ãÎÑ§\n",
    "\n",
    "t_indices = target.input_ids.unsqueeze(axis=-1) #(1, 512, 1)\n",
    "t_logit_of_input_ids = torch.gather(t_outputs, 2, s_indices).squeeze() #(1, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spanÍ∏∏Ïù¥ÎßåÌÅºÏùò logit Ìï©ÏúºÎ°ú Íµ¨ÏÑ±Îêú matrixÍµ¨ÌïòÍ∏∞\n",
    "span_size = 4\n",
    "source_sep_id = (source.input_ids[0] == tokenizer.sep_token_id).nonzero().squeeze()[0]\n",
    "s_n_gram_logits = torch.tensor([sum(s_logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])\n",
    "t_n_gram_logits = torch.tensor([sum(t_logit_of_input_ids[i : i+span_size]) for i in range(0, source_sep_id - span_size + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13)\n"
     ]
    }
   ],
   "source": [
    "# logit Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÌÅ∞ span index Ï∞æÍ∏∞\n",
    "diff = s_n_gram_logits-t_n_gram_logits\n",
    "mask_idx = diff.argmax() #source indexÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ Îê®.\n",
    "print(mask_idx)\n",
    "\n",
    "# logitÏù¥ Í∞ÄÏû• ÌÅ∞ indexÏóê maskingÌïòÍ∏∞\n",
    "source.input_ids[0, mask_idx : mask_idx+span_size] = tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  5050,  2342,     8,   516,  1889,     9,  1540,    28,    81,\n",
       "           670, 10011,    11,     4,     4,     4,     4, 10011,    11,     3,\n",
       "           440,   320,   113,  1284,    81,   219,    10,   159,   157,   607,\n",
       "          5136, 12137,   107,   124,  5050,  5136,   516,  1889,     9,  1896,\n",
       "            26,    30,     5,   741,  1612,   565,   159,   157,   607,  4208,\n",
       "             9,  6076,  6426,   879,   833,    78,  2342,     7,   366,  1612,\n",
       "          1087,   159,   157,   607,  5136,   301,    10,  1941,    87,    68,\n",
       "           284,  1340,   159,    16,  6034,  1027,  3005,     5,   159,   157,\n",
       "           607,  1540,  6426,  3555,  2817,    72,   266,  5285,  1153,   277,\n",
       "          2597,  1505,  3773,   440,   320,   113,    10,   912,   359,    32,\n",
       "          2270,  1427,  1540,     6,  1844,    13,  1043,  4974,     9,   658,\n",
       "           464,   681,  1436,   369,   605,    16,  1219,  6068,    16,  6402,\n",
       "          2232,  1599,    21,  1574,     5,  1540,  3817,    21,   285,    13,\n",
       "           501,   372,  2916,    22,   833,    78,  6289,    10,   388,   785,\n",
       "           757,  1540,     6,    87,    68,   128,  9925,    27,  7809,   278,\n",
       "          2088,  2236,    15,  2334,    20,   109,  1679,     5,   131,  6002,\n",
       "            28,   229,  3062,    24, 13914,   215,  5136,  7424,   170,   741,\n",
       "          1612,    69,   159,   157,   607,  4208,     9,  6076,  6426,  6215,\n",
       "            45,  1748,   878,     5,  1286,   333,     7,   366,   301,   741,\n",
       "          1612,   565,   159,   157,   607,  4208,     9,  6076,  6426,  3285,\n",
       "            68,   440,   109,  1679,     5,   660,    25,  1781,    68,   211,\n",
       "          2993,   508,  1991,  1941,   102,    68,  3585,     5,   367,    13,\n",
       "           741,  1612,    69,   159,   157,   607,  4208,     9,  6076,  3991,\n",
       "            15,  2741,   727,    68,   300,  1469,   446,   109,  6761,     5,\n",
       "          1286,   333,    15,   301,   229,  3062,     6,  3365,   146,   208,\n",
       "           303,  2312,    10,  1732,  1651,    64,   367,   120,   426,  1627,\n",
       "          2312,    19,   643,    68,   190,  6944,  1540,    10,  4319,  1147,\n",
       "             5, 13914,   215,     7,   366,   301,  3365,   146,  4299,  1295,\n",
       "          2312,    23,   367,   187,  1276,  2312,    19,   614,    68,  4035,\n",
       "          1896,   124,   159,    16,  6034,  5815,   139,  3021,   593,     5,\n",
       "           147,  7600,    41,     8,   677,   307,  5285,    57,    21,  5849,\n",
       "            28,  4269,     5,   147,  7600,    41,     7,   366,   301,   741,\n",
       "          1612,   565,   159,   157,   607,  4208,     9,  6076,  3991,    15,\n",
       "          4956,    68, 13348,  1209,   367,    43,  1220,   106,   159,   157,\n",
       "           607,  4208,     9,  6076,  3991,    15,  6212,    68,   234,  3021,\n",
       "           593,     5,   677,   307,  5285,     7,   366,   159,   157,   607,\n",
       "          4208,  6426,   301,  3717,    68,   613,   367,  1322,    68,   284,\n",
       "             6,   390,    47,   109,   839,     5,   455,   677,   307,  5285,\n",
       "             7,   366,   741,  1612,   565,   833,    78,     9,  6076,  6426,\n",
       "          2377,    68,   234,   757,  1540,  1219,   369,  3394,    15,  1655,\n",
       "            47,   109,  6761,     5,  2040,    21,  5390, 10433,  1540,    28,\n",
       "            81, 10434,   159,   157,   607,  4208,  1505,    10,  1338,  4264,\n",
       "           898,  5050,  1153,   277,  7424,    20,   440,   320,  3139,   321,\n",
       "            31,     5,  3514,   118,    22,   118,   146,     7,   366,   741,\n",
       "          1612,    69,  7105,    16,  6076,  3991,    15,   660,    25,   102,\n",
       "            68,   126,    23,  1561,    25,   117,    68,  6263,  1340,   301,\n",
       "           120,    87,    68,   300,  1469,  9977,    31,     5,  1612,  1505,\n",
       "          1255,   230,   440,   320,   113,  6805,  1255,     9,   140,  2872,\n",
       "           878,     5,   455,   440,   320,   113,     7,   366,  1612,    24,\n",
       "          7283,     9,  2597,  1896,    31,  3118,  4402,  1529,   398,    42,\n",
       "          1649,    15,   140,  2555,     5,   674,  1783,    13,   555,   497,\n",
       "            34,     3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ÏãùÌíàÏóÖÍ≥Ñ, Ï§ÑÏ§ÑÏù¥ Í∞ÄÍ≤©Ïù∏ÏÉÅ ‚Ä¶ \\\\ \" [MASK] [MASK] [MASK] [MASK] \\\\ \" [SEP] Ïù∏Í±¥ÎπÑ Í∏âÏÉÅÏäπÏóê ÏõêÏû¨Î£å Í∞ÄÍ≤©Ïù¥ Í≤πÏπòÎ©∞ ÏãùÌíà Í∞ÄÍ≤©Ïù¥ Ï§ÑÏ§ÑÏù¥ Ïò§Î•¥Í≥† ÏûàÎã§. Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ ÎÜíÏùÄ ÎùºÎ©¥ÏóÖÍ≥ÑÏùò Í≤ΩÏö∞ Îß§Ï∂ú ÎåÄÎπÑ ÏõêÏû¨Î£å Í∞ÄÍ≤©Ïù¥ ÏßÄÎÇúÌï¥Ïóê ÎπÑÌï¥ 10 % Ïù¥ÏÉÅ Ïò¨Îùº ÏõêÍ∞Ä Î∂ÄÎã¥Ïù¥ ÌÅ¨Í≤å ÎäòÏóàÎã§. ÏõêÏû¨Î£å Í∞ÄÍ≤© ÎπÑÏ§ëÏù¥ Îã§ÏÜå ÎÇÆÏùÄ Í∞ÄÍ≥µÏãùÌíàÏóÖÏ≤¥Îì§ÏùÄ Îß§ÎÖÑ ÏÉÅÏäπÌï¥Ïò® Ïù∏Í±¥ÎπÑÏóê Î≤ÑÌã∞ÏßÄ Î™ªÌïòÍ≥† ÏÉÅÌíà Í∞ÄÍ≤©ÏùÑ Ïò¨Î¶¨Îäî ÏïÖÏàúÌôòÏù¥ Í≥ÑÏÜçÎêòÍ≥† ÏûàÏñ¥ ÎÇ¥ÎÖÑÏóêÎèÑ Î¨ºÍ∞Ä Ïù∏ÏÉÅ Í∏∞Ï°∞Í∞Ä Ïù¥Ïñ¥Ïßà Í≤ÉÏù¥ÎùºÎäî Ï†ÑÎßùÎèÑ ÎÇòÏò®Îã§. Í∞ÄÍ≤© Ïò¨Î†§ÎèÑ ÎÇ®Îäî Í≤å ÏóÜÎã§24Ïùº ÎùºÎ©¥ ÏóÖÏ≤¥Îì§Ïóê Îî∞Î•¥Î©¥ Ï£ºÏöî Ï†úÌíà Í∞ÄÍ≤©ÏùÑ 10 % ÏïàÌåéÏúºÎ°ú Ïò¨Î†∏ÏßÄÎßå Ïó¨Ï†ÑÌûà Ïã§Ï†ÅÏùÄ Î∂ÄÏßÑÌïú Í≤ÉÏúºÎ°ú ÎÇòÌÉÄÎÇ¨Îã§. Ï£º Ïû¨Î£åÏù∏ ÏÜåÎß•Í≥º ÌåúÏú† Í∞ÄÍ≤©Ïù¥ Í∏âÎì±ÌïòÎ©∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ Í∏âÏ¶ùÌïòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÎÜçÏã¨Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ 43 % Ïù∏ Í≤ÉÏúºÎ°ú ÎÇòÌÉÄÎÇ¨Îã§. 2018ÎÖÑ 35 % Ïóê Î∂àÍ≥ºÌñàÎçò Í≤ÉÏóê ÎπÑÌï¥ 8 % Ï¶ùÍ∞ÄÌñàÎã§. Ïò¨Ìï¥Îäî Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ ÏµúÏÜå 50 % Î•º ÎÑòÏñ¥ÏÑ§ Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÎÜçÏã¨ÏùÄ ÏßÄÎÇúÌï¥ ÏÜåÎß•ÏùÑ tÎãπ 202Îã¨Îü¨Ïóê ÏàòÏûÖÌñàÏúºÎÇò Ïò¨Ìï¥ÏóêÎäî 258Îã¨Îü¨Î°ú 27 % ÎπÑÏãº Í∞ÄÍ≤©Ïóê Îì§Ïó¨ÏôîÎã§. ÌåúÏú†Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ tÎãπ 627Îã¨Îü¨ÏóêÏÑú Ïò¨Ìï¥ 1110Îã¨Îü¨Î°ú 100 % Í∞ÄÍπåÏù¥ Ïò§Î•¥Î©∞ ÏõêÍ∞Ä Î∂ÄÎã¥Ïù¥ Í∏âÍ≤©Ìûà ÎÜíÏïÑÏ°åÎã§. Ïò§ÎöúÍ∏∞, ÏÇºÏñëÏãùÌíà Îì±ÎèÑ ÎßàÏ∞¨Í∞ÄÏßÄÏù∏ ÏÉÅÌô©Ïù¥Îã§. Ïò§ÎöúÍ∏∞Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 69 % ÏòÄÏúºÎÇò Ïò¨Ìï¥ 3Î∂ÑÍ∏∞ÍπåÏßÄ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 81 % Î°ú ÎÜíÏïÑÏ°åÎã§. ÏÇºÏñëÏãùÌíàÏùò Í≤ΩÏö∞ ÏõêÏû¨Î£å Í∞í ÎπÑÏ§ëÏù¥ ÏßÄÎÇúÌï¥ 56 % ÏóêÏÑú Ïò¨Ìï¥ 60 % Ïù¥ÏÉÅÏùÑ Í∏∞Î°ùÌï† Í≤ÉÏúºÎ°ú Î≥¥Ïù∏Îã§. ÌäπÌûà ÏÇºÏñëÏãùÌíàÏùò Í≤ΩÏö∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÎùºÎ©¥Ïù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ 90 % Î°ú Ï†úÌíà Í∞ÄÍ≤© Ïù∏ÏÉÅÏóêÎèÑ ÏòÅÏóÖÏù¥ÏùµÏùÄ ÌïòÎùΩÌï† Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÎÑàÎèÑ ÎÇòÎèÑ ‚Äò Í∞ÄÍ≤©Ïù∏ÏÉÅ ‚Äô ÏõêÏû¨Î£å Í∞í ÏÉÅÏäπÏóê ÏòÅÌñ•ÏùÑ Îçú Î∞õÏùÄ ÏãùÌíàÏóÖÏ≤¥Îì§ÏùÄ Í∏âÎì±Ìïú Ïù∏Í±¥ÎπÑÍ∞Ä Î¨∏Ï†úÎã§. CJÏ†úÏùºÏ†úÎãπÏùò Í≤ΩÏö∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë Í∏âÏó¨Í∞Ä Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 2018ÎÖÑ 8 % ÎåÄÏóêÏÑú 2019ÎÖÑ 9 % ÎåÄÎ°ú Ïò¨Îùº ÏßÄÎÇúÌï¥ÏóêÎäî 10 % Î•º ÎÑòÏñ¥ÏÑ∞Îã§. Îß§Ï∂ú ÏÉÅÏäπÌè≠Î≥¥Îã§ Ïù∏Í±¥ÎπÑ ÏßÄÏ∂úÌè≠Ïù¥ Îçî ÌÅ¨Í∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÌäπÌûà Ïù∏Í±¥ÎπÑÏùò Í≤ΩÏö∞ Îß§Ï∂úÍ≥º ÏÉÅÍ¥ÄÏóÜÏù¥ Îß§ÎÖÑ Ïò§Î•¥Îã§ Î≥¥Îãà Í∏∞ÏóÖÏù¥ Ï≤¥Í∞êÌïòÎäî Î∂ÄÎã¥ÏùÄ Îçî ÌÅ¨Îã§. Î°ØÎç∞Î¶¨ÏïÑÎäî Îã§Ïùå Îã¨ 1 [SEP]'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(source.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_gen = source.copy()\n",
    "data_for_gen[\"token_type_ids\"] = torch.tensor([1] * cfg[\"MODEL\"][\"max_seq_length\"], dtype = torch.long) #target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**data_for_gen).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÎÇ¥ÎÖÑÎèÑ [PAD] [PAD]'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve index of [MASK]\n",
    "mask_token_index = (data_for_gen.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = outputs[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id) #ÌïôÏäµÏù¥ Ï†úÎåÄÎ°ú ÏïàÎê®... ÎÇò Ïñ¥Îñ†Ïπ¥ÏßÄ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ÏãùÌíàÏóÖÍ≥Ñ, Ï§ÑÏ§ÑÏù¥ Í∞ÄÍ≤©Ïù∏ÏÉÅ ‚Ä¶ \\\\ \" ÎÇ®Îäî Í≤å ÏóÜÎã§ \\\\ \" [SEP] Ïù∏Í±¥ÎπÑ Í∏âÏÉÅÏäπÏóê ÏõêÏû¨Î£å Í∞ÄÍ≤©Ïù¥ Í≤πÏπòÎ©∞ ÏãùÌíà Í∞ÄÍ≤©Ïù¥ Ï§ÑÏ§ÑÏù¥ Ïò§Î•¥Í≥† ÏûàÎã§. Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ ÎÜíÏùÄ ÎùºÎ©¥ÏóÖÍ≥ÑÏùò Í≤ΩÏö∞ Îß§Ï∂ú ÎåÄÎπÑ ÏõêÏû¨Î£å Í∞ÄÍ≤©Ïù¥ ÏßÄÎÇúÌï¥Ïóê ÎπÑÌï¥ 10 % Ïù¥ÏÉÅ Ïò¨Îùº ÏõêÍ∞Ä Î∂ÄÎã¥Ïù¥ ÌÅ¨Í≤å ÎäòÏóàÎã§. ÏõêÏû¨Î£å Í∞ÄÍ≤© ÎπÑÏ§ëÏù¥ Îã§ÏÜå ÎÇÆÏùÄ Í∞ÄÍ≥µÏãùÌíàÏóÖÏ≤¥Îì§ÏùÄ Îß§ÎÖÑ ÏÉÅÏäπÌï¥Ïò® Ïù∏Í±¥ÎπÑÏóê Î≤ÑÌã∞ÏßÄ Î™ªÌïòÍ≥† ÏÉÅÌíà Í∞ÄÍ≤©ÏùÑ Ïò¨Î¶¨Îäî ÏïÖÏàúÌôòÏù¥ Í≥ÑÏÜçÎêòÍ≥† ÏûàÏñ¥ ÎÇ¥ÎÖÑÏóêÎèÑ Î¨ºÍ∞Ä Ïù∏ÏÉÅ Í∏∞Ï°∞Í∞Ä Ïù¥Ïñ¥Ïßà Í≤ÉÏù¥ÎùºÎäî Ï†ÑÎßùÎèÑ ÎÇòÏò®Îã§. Í∞ÄÍ≤© Ïò¨Î†§ÎèÑ ÎÇ®Îäî Í≤å ÏóÜÎã§24Ïùº ÎùºÎ©¥ ÏóÖÏ≤¥Îì§Ïóê Îî∞Î•¥Î©¥ Ï£ºÏöî Ï†úÌíà Í∞ÄÍ≤©ÏùÑ 10 % ÏïàÌåéÏúºÎ°ú Ïò¨Î†∏ÏßÄÎßå Ïó¨Ï†ÑÌûà Ïã§Ï†ÅÏùÄ Î∂ÄÏßÑÌïú Í≤ÉÏúºÎ°ú ÎÇòÌÉÄÎÇ¨Îã§. Ï£º Ïû¨Î£åÏù∏ ÏÜåÎß•Í≥º ÌåúÏú† Í∞ÄÍ≤©Ïù¥ Í∏âÎì±ÌïòÎ©∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ Í∏âÏ¶ùÌïòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÎÜçÏã¨Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ 43 % Ïù∏ Í≤ÉÏúºÎ°ú ÎÇòÌÉÄÎÇ¨Îã§. 2018ÎÖÑ 35 % Ïóê Î∂àÍ≥ºÌñàÎçò Í≤ÉÏóê ÎπÑÌï¥ 8 % Ï¶ùÍ∞ÄÌñàÎã§. Ïò¨Ìï¥Îäî Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ ÏµúÏÜå 50 % Î•º ÎÑòÏñ¥ÏÑ§ Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÎÜçÏã¨ÏùÄ ÏßÄÎÇúÌï¥ ÏÜåÎß•ÏùÑ tÎãπ 202Îã¨Îü¨Ïóê ÏàòÏûÖÌñàÏúºÎÇò Ïò¨Ìï¥ÏóêÎäî 258Îã¨Îü¨Î°ú 27 % ÎπÑÏãº Í∞ÄÍ≤©Ïóê Îì§Ïó¨ÏôîÎã§. ÌåúÏú†Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ tÎãπ 627Îã¨Îü¨ÏóêÏÑú Ïò¨Ìï¥ 1110Îã¨Îü¨Î°ú 100 % Í∞ÄÍπåÏù¥ Ïò§Î•¥Î©∞ ÏõêÍ∞Ä Î∂ÄÎã¥Ïù¥ Í∏âÍ≤©Ìûà ÎÜíÏïÑÏ°åÎã§. Ïò§ÎöúÍ∏∞, ÏÇºÏñëÏãùÌíà Îì±ÎèÑ ÎßàÏ∞¨Í∞ÄÏßÄÏù∏ ÏÉÅÌô©Ïù¥Îã§. Ïò§ÎöúÍ∏∞Ïùò Í≤ΩÏö∞ ÏßÄÎÇúÌï¥ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 69 % ÏòÄÏúºÎÇò Ïò¨Ìï¥ 3Î∂ÑÍ∏∞ÍπåÏßÄ ÏõêÏû¨Î£å Í∞íÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 81 % Î°ú ÎÜíÏïÑÏ°åÎã§. ÏÇºÏñëÏãùÌíàÏùò Í≤ΩÏö∞ ÏõêÏû¨Î£å Í∞í ÎπÑÏ§ëÏù¥ ÏßÄÎÇúÌï¥ 56 % ÏóêÏÑú Ïò¨Ìï¥ 60 % Ïù¥ÏÉÅÏùÑ Í∏∞Î°ùÌï† Í≤ÉÏúºÎ°ú Î≥¥Ïù∏Îã§. ÌäπÌûà ÏÇºÏñëÏãùÌíàÏùò Í≤ΩÏö∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Í∞ÄÏö¥Îç∞ ÎùºÎ©¥Ïù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏù¥ 90 % Î°ú Ï†úÌíà Í∞ÄÍ≤© Ïù∏ÏÉÅÏóêÎèÑ ÏòÅÏóÖÏù¥ÏùµÏùÄ ÌïòÎùΩÌï† Í≤ÉÏúºÎ°ú Ï†ÑÎßùÎêúÎã§. ÎÑàÎèÑ ÎÇòÎèÑ ‚Äò Í∞ÄÍ≤©Ïù∏ÏÉÅ ‚Äô ÏõêÏû¨Î£å Í∞í ÏÉÅÏäπÏóê ÏòÅÌñ•ÏùÑ Îçú Î∞õÏùÄ ÏãùÌíàÏóÖÏ≤¥Îì§ÏùÄ Í∏âÎì±Ìïú Ïù∏Í±¥ÎπÑÍ∞Ä Î¨∏Ï†úÎã§. CJÏ†úÏùºÏ†úÎãπÏùò Í≤ΩÏö∞ Ï†ÑÏ≤¥ Îß§Ï∂ú Ï§ë Í∏âÏó¨Í∞Ä Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ 2018ÎÖÑ 8 % ÎåÄÏóêÏÑú 2019ÎÖÑ 9 % ÎåÄÎ°ú Ïò¨Îùº ÏßÄÎÇúÌï¥ÏóêÎäî 10 % Î•º ÎÑòÏñ¥ÏÑ∞Îã§. Îß§Ï∂ú ÏÉÅÏäπÌè≠Î≥¥Îã§ Ïù∏Í±¥ÎπÑ ÏßÄÏ∂úÌè≠Ïù¥ Îçî ÌÅ¨Í∏∞ ÎïåÎ¨∏Ïù¥Îã§. ÌäπÌûà Ïù∏Í±¥ÎπÑÏùò Í≤ΩÏö∞ Îß§Ï∂úÍ≥º ÏÉÅÍ¥ÄÏóÜÏù¥ Îß§ÎÖÑ Ïò§Î•¥Îã§ Î≥¥Îãà Í∏∞ÏóÖÏù¥ Ï≤¥Í∞êÌïòÎäî Î∂ÄÎã¥ÏùÄ Îçî ÌÅ¨Îã§. Î°ØÎç∞Î¶¨ÏïÑÎäî Îã§Ïùå Îã¨ 1 [SEP]'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (17) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m labels \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39mThe capital of France is Paris.\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# mask labels of non-[MASK] tokens\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(target\u001b[39m.\u001b[39;49minput_ids[\u001b[39m0\u001b[39;49m] \u001b[39m==\u001b[39;49m tokenizer\u001b[39m.\u001b[39;49mmask_token_id, labels, \u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtarget, labels\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f626169742d6e6577732d67656e32222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e3137362e313434227d7d/workspace/code/Bait-News-Generation/Fake-News-Detection-Dataset/exp/paddedMLM.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mround\u001b[39m(outputs\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mitem(), \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (17) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(target.input_ids[0] == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**target, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
